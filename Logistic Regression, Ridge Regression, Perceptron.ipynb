{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1dkS0VksGp7"
      },
      "source": [
        "# CIS 5200: Machine Learning\n",
        "## Homework 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXZxyZDbr-Yx",
        "outputId": "b917cb5d-ed80-4fbd-b258-279aa545a706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO, OK] Google Colab.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# For autograder only, do not modify this cell. \n",
        "# True for Google Colab, False for autograder\n",
        "NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n",
        "if NOTEBOOK:\n",
        "    print(\"[INFO, OK] Google Colab.\")\n",
        "else:\n",
        "    print(\"[INFO, OK] Autograder.\")\n",
        "    sys.exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swECpqQGvLu9"
      },
      "source": [
        "### Penngrader setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-peqcQNCvFSS",
        "outputId": "38db7c70-a9f4-4200-d309-8b237fd15093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: penngrader-client in /usr/local/lib/python3.8/dist-packages (0.5.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from penngrader-client) (6.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from penngrader-client) (0.3.6)\n"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "!pip install penngrader-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VOzgVapPgrZ",
        "outputId": "ed0e3be4-a1fb-4336-a88e-6b389f12c619"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting config.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.yaml\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSSxUlaHvsrK",
        "outputId": "8802974f-c0fc-479e-92c8-21d004e88dfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PennGrader initialized with Student ID: 38513713\n",
            "\n",
            "Make sure this correct or we will not be able to store your grade\n"
          ]
        }
      ],
      "source": [
        "from penngrader.grader import PennGrader\n",
        "\n",
        "# PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO \n",
        "# TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 38513713 # YOUR PENN-ID GOES HERE AS AN INTEGER #\n",
        "SECRET = STUDENT_ID\n",
        "\n",
        "grader = PennGrader('config.yaml', 'CIS5200_23Sp_HW1', STUDENT_ID, SECRET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07cBnaLxrBC9"
      },
      "source": [
        "# Dataset: Wine Quality Prediction\n",
        "\n",
        "Some research on blind wine tasting has suggested that [people cannot taste the difference between ordinary and pricy wine brands](https://phys.org/news/2011-04-expensive-inexpensive-wines.html). Indeed, even experienced tasters may be as consistent as [random numbers](https://www.seattleweekly.com/food/wine-snob-scandal/). Is professional wine tasting in shambles? Maybe ML can take over. \n",
        "\n",
        "In this problem set, we will train some simple linear models to predict wine quality. We'll be using the data from [this repository](https://archive.ics.uci.edu/ml/datasets/Wine+Quality) for both the classification and regression tasks. The following cells will download and set up the data for you. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "OxMFoDHQrAhj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "SfW-vxHn_209"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "red_df = pd.read_csv('winequality-red.csv', delimiter=';')\n",
        "\n",
        "X = torch.from_numpy(red_df.drop(columns=['quality']).to_numpy())\n",
        "y = torch.from_numpy(red_df['quality'].to_numpy())\n",
        "\n",
        "# Split data into train/test splits \n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42) \n",
        "\n",
        "# Normalize the data to have zero mean and standard deviation, \n",
        "# and add bias term\n",
        "mu, sigma = X_train.mean(0), X_train.std(0) \n",
        "X_train, X_test = [ torch.cat([((x-mu)/sigma).float(), torch.ones(x.size(0),1)], dim=1) \n",
        "                    for x in [X_train, X_test]]\n",
        "\n",
        "# Transform labels to {-1,1} for logistic regression\n",
        "y_binary_train, y_binary_test = [ (torch.sign(y - 5.5)).long() \n",
        "                                  for y in [y_train, y_test]]\n",
        "y_regression_train, y_regression_test = [ y.float() for y in [y_train, y_test]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekkAUw53m9_W"
      },
      "source": [
        "# 1. Logistic Regression\n",
        "\n",
        "In this first problem, you will implement a logistic regression classifier to classify good wine (`y=1`) from bad wine (`y=-1`). Your professor has arbitrarily decided that good wine has a score of at least 5.5. The classifier is split into the following components: \n",
        "\n",
        "1. Loss (3pts) & gradient (3pts) - given a batch of examples $X$ and labels $y$ and weights for the logistic regression classifier, compute the batched logistic loss and gradient of the loss with *with respect to the model parameters $w$*. Note that this is slightly different from the gradient in Homework 0, which was with respect to the sample $X$. \n",
        "2. Fit (2pt) - Given a loss function and data, find the weights of an optimal logistic regression model that minimizes the logistic loss\n",
        "3. Predict (3pts) - Given the weights of a logistic regression model and new data, predict the most likely class\n",
        "\n",
        "We provide an generic gradient-based optimizer for you which minimizes the logistic loss function, you can call it with `LogisticOptimizer().optimize(X,y)`. It does not need any parameter adjustment. \n",
        "\n",
        "Hint: The optimizer will minimize the logistic loss. So this value of this loss should be decreasing over iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "eBqvZesupS5k"
      },
      "outputs": [],
      "source": [
        "class LogisticOptimizer: \n",
        "    @staticmethod\n",
        "    def logistic_loss(X, y, w): \n",
        "        # Given a batch of samples and labels, and the weights of a logistic \n",
        "        # classifier, compute the batched logistic loss. \n",
        "        # \n",
        "        # X := Tensor(float) of size (m,d) --- This is a batch of m examples of \n",
        "        #     of dimension d\n",
        "        # \n",
        "        # y := Tensor(int) of size (m,) --- This is a batch of m labels in {-1,1}\n",
        "        # \n",
        "        # w := Tensor(float) of size(d,) --- This is the weights of a logistic \n",
        "        #     classifer. \n",
        "        # \n",
        "        # Return := Tensor of size (m,) --- This is the logistic loss for each \n",
        "        #     example. \n",
        "\n",
        "        # Fill in the rest\n",
        "        # log \u00101 + exp(−y w⊤x)\n",
        "        y = y.float()\n",
        "        fx = torch.sigmoid(w.unsqueeze(1).T@X.T)\n",
        "        ret = torch.where(y == 1, -torch.log(fx), -torch.log(1 - fx))\n",
        "        return ret.squeeze(0)\n",
        "\n",
        "    @staticmethod\n",
        "    def logistic_gradient(X, y, w): \n",
        "        # Given a batch of samples and labels, compute the batched gradient of\n",
        "        # the logistic loss. \n",
        "        # \n",
        "        # X := Tensor(float) of size (m,d) --- This is a batch of m examples of \n",
        "        #     of dimension d\n",
        "        # \n",
        "        # y := Tensor(int) of size (m,) --- This is a batch of m labels in {-1,1}\n",
        "        # \n",
        "        # w := Tensor(float) of size(d,) --- This is the weights of a logistic \n",
        "        #     classifer. \n",
        "        # \n",
        "        # Return := Tensor of size (m,d) --- This is the logistic gradient for each \n",
        "        #     example. \n",
        "        # \n",
        "        # Hint: A very similar gradient was calculated in Homework 0. \n",
        "        # However, that was the sample gradient (with respect to X), whereas \n",
        "        # what we need here is the parameter gradient (with respect to w). \n",
        "    \n",
        "        # Fill in the rest\n",
        "        ywx = y*X.matmul(w)\n",
        "        return X * (-y*torch.exp(-ywx)/(1 + torch.exp(-ywx))).unsqueeze(1)\n",
        "\n",
        "    def optimize(self, X, y, niters=100): \n",
        "        # Given a dataset of examples and labels, minimizes the logistic loss \n",
        "        # using standard gradient descent. \n",
        "        # \n",
        "        # This optimizer is written for you, and you only need to implement the \n",
        "        # logistic loss and gradient functions above. \n",
        "        # \n",
        "        # X := Tensor(float) of size (m,d) --- This is a batch of m examples of \n",
        "        #     of dimension d\n",
        "        # \n",
        "        # y := Tensor(int) of size (m,) --- This is a batch of m labels in {-1,1}\n",
        "        # \n",
        "        # Return := Tensor of size(d,) --- This is the fitted weights of a \n",
        "        #     logistic regression model \n",
        "\n",
        "        m,d = X.size()\n",
        "        w = torch.zeros(d)\n",
        "        print('Optimizing logistic function...')\n",
        "        for i in range(niters): \n",
        "            loss = self.logistic_loss(X,y,w).mean()\n",
        "            grad = self.logistic_gradient(X,y,w).mean(0)\n",
        "            w -= grad\n",
        "            if i % 50 == 0: \n",
        "                print(i, loss.item())\n",
        "        print('Optimizing done.')\n",
        "        return w\n",
        "\n",
        "def logistic_fit(X, y, optimizer=LogisticOptimizer): \n",
        "    # Given a dataset of examples and labels, fit the weights of the logistic \n",
        "    # regression classifier using the provided loss function and optimizer\n",
        "    # \n",
        "    # X := Tensor(float) of size (m,d) --- This is a batch of m examples of \n",
        "    #     of dimension d\n",
        "    # \n",
        "    # y := Tensor(int) of size (m,) --- This is a batch of m labels in {-1,1}\n",
        "    # \n",
        "    # Return := Tensor of size (d,) --- This is the fitted weights of the \n",
        "    #     logistic regression model \n",
        "    opt = LogisticOptimizer()\n",
        "\n",
        "    # Fill in the rest\n",
        "    return LogisticOptimizer.optimize(opt, X, y)\n",
        "\n",
        "def logistic_predict(X, w):\n",
        "    # Given a dataset of examples and fitted weights for a logistic regression \n",
        "    # classifier, predict the class\n",
        "    # \n",
        "    # X := Tensor(float) of size(m,d) --- This is a batch of m examples of \n",
        "    #    dimension d\n",
        "    # \n",
        "    # w := Tensor(float) of size (d,) --- This is the fitted weights of the \n",
        "    #    logistic regression model\n",
        "    # \n",
        "    # Return := Tensor of size (m,) --- This is the predicted classes {-1,1}\n",
        "    #    for each example\n",
        "    #\n",
        "    # Hint: Remember that logistic regression expects a label in {-1,1}, and \n",
        "    # not {0,1} \n",
        "\n",
        "    # Fill in the rest\n",
        "    return torch.sign(X.matmul(w))\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJziPLnPtFH-",
        "outputId": "8d70aa3d-f99d-4c9b-a35d-5c9d5b005ea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizing logistic function...\n",
            "0 0.6931473016738892\n",
            "50 0.518214225769043\n",
            "Optimizing done.\n",
            "Train accuracy [zero]: 0.00\n",
            "Test accuracy [zero]: 0.00\n",
            "Train accuracy [random]: 0.54\n",
            "Test accuracy [random]: 0.53\n",
            "Train accuracy [fitted]: 0.75\n",
            "Test accuracy [fitted]: 0.75\n"
          ]
        }
      ],
      "source": [
        "# Test your code on the wine dataset! \n",
        "# How does your solution compare to a random linear classifier? \n",
        "# Your solution should get around 75% accuracy on the test set. \n",
        "torch.manual_seed(42)\n",
        "\n",
        "d = X_train.size(1)\n",
        "logistic_weights = {\n",
        "    'zero': torch.zeros(d), \n",
        "    'random': torch.randn(d),\n",
        "    'fitted': logistic_fit(X_train, y_binary_train)\n",
        "}\n",
        "\n",
        "for k,w in logistic_weights.items(): \n",
        "    yp_binary_train = logistic_predict(X_train, w)\n",
        "    acc_train = (yp_binary_train == y_binary_train).float().mean()\n",
        "\n",
        "    print(f'Train accuracy [{k}]: {acc_train.item():.2f}')\n",
        "\n",
        "    yp_binary_test = logistic_predict(X_test, w)\n",
        "    acc_test = (yp_binary_test == y_binary_test).float().mean()\n",
        "\n",
        "    print(f'Test accuracy [{k}]: {acc_test.item():.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KniRjDnBj6cN"
      },
      "source": [
        "### Autograder\n",
        "Be sure you can pass the following four tests! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byU-tJrxVbO4",
        "outputId": "aaf0a151-f805-40bf-ee19-bc869ac2f23b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You earned 2/3 points.\n",
            "\n",
            "But, don't worry you can re-submit and we will keep only your latest score.\n",
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "grader.grade(test_case_id = 'logistic_loss', answer = LogisticOptimizer.logistic_loss)\n",
        "grader.grade(test_case_id = 'logistic_gradient', answer = LogisticOptimizer.logistic_gradient)\n",
        "grader.grade(test_case_id = 'logistic_fit', answer = logistic_fit)\n",
        "grader.grade(test_case_id = 'logistic_predict', answer = logistic_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQEG5lsN4EiQ"
      },
      "source": [
        "# 2. Linear Regression with Ridge Regression\n",
        "\n",
        "In this second problem, you'll implement a linear regression model. Similarly to the first problem, implement the following functions: \n",
        "\n",
        "1. Loss (3pts) - Given a batch of examples $X$ and labels $y$, compute the batched mean squared error loss for a linear model with weights $w$. \n",
        "2. Fit (4pts) - Given a batch of examples $X$ and labels $y$, find the weights of the optimal linear regression model\n",
        "3. Predict (3pts) - Given the weights $w$ of a linear regression model and new data $X$, predict the most likely label\n",
        "\n",
        "This time, you are not given an optimizer for the fitting function since this problem has an analytic solution. Make sure to test your solution with non-zero ridge regression parameters. \n",
        "\n",
        "Hint: You may want to review ridge regression on Slide 22 of Lecture 3. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1Qwk_gQuvvUD"
      },
      "outputs": [],
      "source": [
        "def regression_loss(X, y, w): \n",
        "    # Given a batch of linear regression outputs and true labels, compute \n",
        "    # the batch of squared error losses. This is *without* the ridge \n",
        "    # regression penalty. \n",
        "    # \n",
        "    # X := Tensor(float) of size (m,d) --- This is a batch of m examples of \n",
        "    #     of dimension d\n",
        "    # \n",
        "    # y := Tensor(int) of size (m,) --- This is a batch of m real-valued labels\n",
        "    # \n",
        "    # w := Tensor(float) of size(d,) --- This is the weights of a linear \n",
        "    #     classifer\n",
        "    # \n",
        "    # Return := Tensor of size (m,) --- This is the squared loss for each \n",
        "    #     example\n",
        "\n",
        "    # Fill in the rest\n",
        "    y_pred = w@X.T\n",
        "    return (y-y_pred)**2\n",
        "\n",
        "def regression_fit(X, y, ridge_penalty=1.0): \n",
        "    # Given a dataset of examples and labels, fit the weights of the linear \n",
        "    # regression classifier using the provided loss function and optimizer\n",
        "    # \n",
        "    # X := Tensor(float) of size (m,d) --- This is a batch of m examples of \n",
        "    #     of dimension d\n",
        "    # \n",
        "    # y := Tensor(float) of size (m,) --- This is a batch of m real-valued \n",
        "    #     labels\n",
        "    #\n",
        "    # ridge_penalty := float --- This is the parameter for ridge regression\n",
        "    # \n",
        "    # Return := Tensor of size (d,) --- This is the fitted weights of the \n",
        "    #     linear regression model \n",
        "    # \n",
        "    # Fill in the rest\n",
        "    # wˆλ = (X⊤X + λmI)−1X⊤Y.\n",
        "    m = X.size(0)\n",
        "    inv = torch.linalg.inv(X@X.T + ridge_penalty * m * torch.eye(m))\n",
        "    return torch.t(inv@X)@y\n",
        "\n",
        "def regression_predict(X, w):\n",
        "    # Given a dataset of examples and fitted weights for a linear regression \n",
        "    # classifier, predict the label\n",
        "    # \n",
        "    # X := Tensor(float) of size(m,d) --- This is a batch of m examples of \n",
        "    #    dimension d\n",
        "    # \n",
        "    # w := Tensor(float) of size (d,) --- This is the fitted weights of the \n",
        "    #    linear regression model\n",
        "    # \n",
        "    # Return := Tensor of size (m,) --- This is the predicted real-valued labels\n",
        "    #    for each example\n",
        "    # \n",
        "    # Fill in the rest\n",
        "    return w@X.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-IgJEQvmy0n",
        "outputId": "6cb71448-8c4f-4432-ffd0-d49fe563d8b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train accuracy [zero]: 32.28\n",
            "Test accuracy [zero]: 32.97\n",
            "Train accuracy [random]: 29.64\n",
            "Test accuracy [random]: 29.55\n",
            "Train accuracy [fitted]: 8.37\n",
            "Test accuracy [fitted]: 8.60\n"
          ]
        }
      ],
      "source": [
        "# Test your code on the wine dataset! \n",
        "# How does your solution compare to a random linear classifier? \n",
        "# Your solution should get an average squard error of about 8.6 test set. \n",
        "torch.manual_seed(42)\n",
        "\n",
        "d = X_train.size(1)\n",
        "regression_weights = {\n",
        "    'zero': torch.zeros(d), \n",
        "    'random': torch.randn(d),\n",
        "    'fitted': regression_fit(X_train, y_regression_train)\n",
        "}\n",
        "\n",
        "for k,w in regression_weights.items(): \n",
        "    yp_regression_train = regression_predict(X_train, w)\n",
        "    squared_loss_train = regression_loss(X_train, y_regression_train, w).mean()\n",
        "\n",
        "    print(f'Train accuracy [{k}]: {squared_loss_train.item():.2f}')\n",
        "\n",
        "    yp_regression_test = regression_predict(X_test, w)\n",
        "    squared_loss_test = regression_loss(X_test, y_regression_test, w).mean()\n",
        "\n",
        "    print(f'Test accuracy [{k}]: {squared_loss_test.item():.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-ULL5s3o4D7"
      },
      "source": [
        "### Autograder\n",
        "Check the following 3 test cases. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1kFPqYcsXDN",
        "outputId": "331d6f4b-29f8-42c4-e3dd-94fa38f7fcc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 4/4 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "grader.grade(test_case_id = 'regression_loss', answer = regression_loss)\n",
        "grader.grade(test_case_id = 'regression_fit', answer = regression_fit)\n",
        "grader.grade(test_case_id = 'regression_predict', answer = regression_predict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAkrLz3rp4yM"
      },
      "source": [
        "# 3. Perceptron\n",
        "\n",
        "In this last problem, you will implement the perceptron algorithm. Since the algorithm requires linearly separate data and real data is usually noisy, we will have to use a synthetic example. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "vqKUxVIJoDZI"
      },
      "outputs": [],
      "source": [
        "# Data generation for perceptron problem\n",
        "torch.manual_seed(42)\n",
        "\n",
        "m,d = 10,2\n",
        "X_perceptron = torch.randn(m,d) # generate random data\n",
        "w_opt = torch.randn(d) # generate random optimal solution\n",
        "y_perceptron = torch.sign(X_perceptron.matmul(w_opt)) # generate solution-consistent labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_Ottsa3tXYj"
      },
      "source": [
        "We've given you a template for the perceptron algorithm. Calculate the following remaining functions: \n",
        "\n",
        "1. (3pts) Calculate the theoretical margin given the optimal linear classifier \n",
        "2. (3pts) Compute the condition for when the perceptron algorithm makes an update on a specific example\n",
        "3. (3pts) Compute the perceptron algorithm update to the weights when the update condition is satisfied\n",
        "\n",
        "You do not need to change `perceptron_algorithm`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "d1Mflw1qtU6r"
      },
      "outputs": [],
      "source": [
        "def perceptron_margin(X, w_opt): \n",
        "    # X := Tensor(float) of size (m,d) --- This is a batch of m examples of \n",
        "    #     of dimension d\n",
        "    # \n",
        "    # w_opt := Tensor(float) of size (d,) --- This is the ground truth linear \n",
        "    #     weights \n",
        "    # \n",
        "    # Return := Tensor(float) of size (1,) --- This is the theoretical margin\n",
        "    # \n",
        "    # Fill in the rest\n",
        "    w = torch.nn.functional.normalize(w_opt.unsqueeze(0), p=2, dim=1)\n",
        "    ret = torch.min(torch.abs(w@X.T))\n",
        "    print(ret.squeeze(0))\n",
        "    return ret.squeeze(0)\n",
        "\n",
        "def perceptron_update_condition(xi, yi, w): \n",
        "    # xi := Tensor(float) of size (d,) --- This is a a single example of \n",
        "    #     of dimension d\n",
        "    # \n",
        "    # yi := Tensor(float) of size (1,) --- This is a single label for the \n",
        "    #     example xi\n",
        "    # \n",
        "    # w := Tensor(float) of size (d,) --- This is the current estimate of the  \n",
        "    #     linear weights \n",
        "    # \n",
        "    # Return := Tensor(bool) of size (1,) --- This is true if the perceptron \n",
        "    #     algorithim will do an update on this example, and false otherwise\n",
        "    # \n",
        "    # Fill in the rest\n",
        "    yi_pred_sign = torch.sign(w@xi)\n",
        "    return torch.where(yi_pred_sign != torch.sign(yi), True, False)\n",
        "\n",
        "def perceptron_update_weight(xi, yi, w): \n",
        "    # xi := Tensor(float) of size (d,) --- This is a a single example of \n",
        "    #     of dimension d\n",
        "    # \n",
        "    # yi := Tensor(float) of size (1,) --- This is a single label for the \n",
        "    #     example xi\n",
        "    # \n",
        "    # w := Tensor(float) of size (d,) --- This is the current estimate of the  \n",
        "    #     linear weights \n",
        "    # \n",
        "    # Return := Tensor(float) of size (d,) --- This is the updated linear  \n",
        "    #     weights after performing the perceptron update\n",
        "    # \n",
        "    # Fill in the rest\n",
        "    # w = torch.nn.functional.normalize(w.unsqueeze(0), p=2, dim=1)\n",
        "    yixi = yi.float().unsqueeze(0)@xi.float().unsqueeze(0)\n",
        "    ret = torch.add(w.float(), yixi.squeeze(0))\n",
        "    return ret.squeeze(0)\n",
        "\n",
        "def perceptron_algorithm(X, y, niters=100):\n",
        "    m,d = X.size()\n",
        "    w = torch.zeros(d)\n",
        "    for t in range(niters):\n",
        "        weight_updated = False\n",
        "        for xi,yi in zip(X,y): \n",
        "            if perceptron_update_condition(xi, yi, w): \n",
        "                w = perceptron_update_weight(xi, yi, w)\n",
        "                weight_updated = True\n",
        "                break\n",
        "        if not weight_updated: \n",
        "            return w,t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZajA_YLw9M-"
      },
      "source": [
        "Try running the perceptron algorithm. Does it terminate in fewer steps than the theoretical bound? Is the final classifier close to the ground truth? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "6XoXjCMguEK6",
        "outputId": "ac4eebdd-9c16-41f9-ddce-06b70f6e0113"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 1.00 in 1 rounds.\n",
            "tensor(0.1362)\n",
            "Margin: 0.14. The perceptron algorithm is guaranteed to terminate in 53.92 rounds.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9acc2abe80>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Test this out on the synthetic data\n",
        "w_perceptron,t = perceptron_algorithm(X_perceptron,y_perceptron)\n",
        "acc_perceptron = (torch.sign(X_perceptron.matmul(w_perceptron)) == y_perceptron).float().mean()\n",
        "\n",
        "print(f\"Accuracy: {acc_perceptron:.2f} in {t} rounds.\")\n",
        "\n",
        "margin = perceptron_margin(X_perceptron,w_opt)\n",
        "print(f\"Margin: {margin:.2f}. The perceptron algorithm is guaranteed to terminate in {1/(margin**2):.2f} rounds.\")\n",
        "\n",
        "# Plot the classifier. Is it close to the the ground truth? \n",
        "plt.scatter(X_perceptron[:,0], X_perceptron[:,1], c=y_perceptron)\n",
        "X_range = torch.linspace(-2,2, 50)\n",
        "plt.plot(X_range, -w_perceptron[0]/w_perceptron[1]*X_range) # estimated\n",
        "plt.plot(X_range, -w_opt[0]/w_opt[1]*X_range) # optimal\n",
        "plt.legend(['Perceptron Algorithm', 'Ground Truth'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGyzcniWxRzi"
      },
      "source": [
        "### Autograder\n",
        "Check the following 3 test cases. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpxvJmEluxVJ",
        "outputId": "7c3868ab-e2c8-42ae-abab-f4949e577217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ],
      "source": [
        "grader.grade(test_case_id = 'perceptron_margin', answer = perceptron_margin)\n",
        "grader.grade(test_case_id = 'perceptron_update_condition', answer = perceptron_update_condition)\n",
        "grader.grade(test_case_id = 'perceptron_update_weight', answer = perceptron_update_weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-jlV0paox45"
      },
      "source": [
        "# Submitting to Gradescope\n",
        "Before submitting to Gradescope, make sure that selecting \"Runtime\" -> \"Restart and run all\" completes all cells without errors. \n",
        "\n",
        "1. Go to the File menu and choose \"Download .ipynb\" and also \"Download .py\". Make sure these files are named homework1.ipynb and homework1.py, respectively\n",
        "2. Go to GradeScope through the canvas page and ensure your class is \"BAN_CIS-5200-001 202310\"\n",
        "3. Select Homework 1\n",
        "4. Upload both files (the .ipynb and the .py)\n",
        "5. PLEASE CHECK THE AUTOGRADER OUTPUT TO ENSURE YOUR SUBMISSION IS PROCESSED CORRECTLY! If this is the case, you should be all set with the programming component of this homework!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
